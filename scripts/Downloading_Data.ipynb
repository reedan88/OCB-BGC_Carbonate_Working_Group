{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90f51523",
   "metadata": {},
   "source": [
    "# Download Data\n",
    "\n",
    "### Purpose\n",
    "This jupyter notebook highlights two different methods for accessing and downloading data from Ocean Observatories Initiative Carbon System instruments. The first method utilizes OOI's API to perform M2M (Machine-2-Machine) queries for data from the OOI THREDDS data server. The second method requests data from OOI's DataExplorer ERDDAP server.\n",
    "\n",
    "#### THREDDs Data\n",
    "The data served up via OpenDAP on OOI THREDDs servers are the same datasets which can be accessed via OOI's Data Portal at https://ooinet.oceanobservatories.org/. This is the source for accessing realtime or near-realtime data from OOI. \n",
    "\n",
    "\n",
    "#### Data Explorer\n",
    "Data Explorer is the new tool for exploring, discovering, and downloading data from OOI. It can be accessed via the web at https://dataexplorer.oceanobservatories.org/. Data Explorer hosts \"gold copy\" versions of OOI datasets, with all the relevant data stream merged into a single unified file. These datasets are hosted on the Data Explorer ERDDAP server at  However, Data Explorer currently only from the Data Explorer website, they currently can't be downloaded from the ERDDAP server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39c3c9c",
   "metadata": {},
   "source": [
    "---\n",
    "## OOINet/THREDDs\n",
    "First, we are going to access and download data from OOI's Data Portal. Then we will do some dataset reprocessing to make the resulting data easier and more intuitive to work with. This portion of the notebook relies on some community tools which have been developed by OOI's Data Team members which simplify interacting with OOI's API. The two tools are the OOINet tool (https://github.com/reedan88/OOINet) and the Data Explorations Modules (https://github.com/oceanobservatories/ooi-data-explorations).\n",
    "\n",
    "This notebook provides an example on how to use the OOINet download tool to perform the following functions:\n",
    "* Search for datasets\n",
    "* Identify desired reference designator\n",
    "* Get the associated metadata for a given reference designator\n",
    "* Request netCDF datasets for a reference designator\n",
    "* Download the netCDF dataset to your local machine\n",
    "\n",
    "The key parameters which the OOI API requires is the \"reference designator.\" A reference designator may be thought of as a type of instrument located at a fixed location and depth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed42fb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os, shutil, sys, time, re, requests, csv, datetime, pytz\n",
    "import time\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15f8509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the OOINet M2M tool\n",
    "sys.path.append(\"/home/areed/Documents/OOI/reedan88/ooinet/ooinet/\")\n",
    "from m2m import M2M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff32742",
   "metadata": {},
   "source": [
    "#### Set OOINet API access\n",
    "In order access and download data from OOINet, need to have an OOINet api username and access token. Those can be found on your profile after logging in to OOINet. Your username and access token should NOT be stored in this notebook/python script (for security). It should be stored in a yaml file, kept in the same directory, named user_info.yaml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed41b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "userinfo = yaml.load(open(\"../user_info.yaml\"), Loader=yaml.SafeLoader)\n",
    "username = userinfo[\"apiname\"]\n",
    "token = userinfo[\"apikey\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26670a95",
   "metadata": {},
   "source": [
    "#### Connect to OOINet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7518ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "OOINet = M2M(username, token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a83ee2",
   "metadata": {},
   "source": [
    "---\n",
    "## Search Datasets\n",
    "First, we can search the available OOI Reference Designators (i.e. \"refdes\" for short) on the following keys: **array**, **node**, **instrument**. Additionally, can request for \"**English_names**\", which will return the descriptive name for the associated array, node, and instrument. Below, we will search for the available CTD instruments on the Pioneer Array Central Surface Mooring.\n",
    "\n",
    "The major caveat with the search is, similar to searching on ERDDAP datasets, the search terms must be partial or full match based on OOI nomenclature. For example, we have to search for \"PCO2\", \"PCO2AA\", or the full instrument name \"04-PCO2AA\" if we are searching for the sea-surface pCO2 sensor. We can't search \"pco2\", \"carbon dioxide\" or other instrument terms.\n",
    "\n",
    "gold_copy = 'http://thredds.dataexplorer.oceanobservatories.org/thredds/catalog/ooigoldcopy/public/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd06c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruments = OOINet.search_datasets(array=\"CP01CNSM\", instrument=\"PCO2W\")\n",
    "instruments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f20b6e9",
   "metadata": {},
   "source": [
    "From the returned list of available instruments above, we can select a particular instrument using its **reference designator** (refdes for short):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34ba746",
   "metadata": {},
   "outputs": [],
   "source": [
    "refdes = \"CP01CNSM-MFD35-05-PCO2WB000\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f47be65",
   "metadata": {},
   "source": [
    "---\n",
    "## Metadata\n",
    "Next, we can query OOINet for the metadata associated with the selected reference designator. The metadata contains such valuable information such as the available methods and streams (which are required to download the data), the particleKeys (the data variable names), and the associated units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff7d346",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = OOINet.get_metadata(refdes)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb59de",
   "metadata": {},
   "source": [
    "#### Sensor Parameters\n",
    "Each instrument returns multiple parameters containing a variety of low-level instrument output and metadata. However, we are interested in science-relevant parameters. We can identify the science parameters based on the preload database, which designates the science parameters with a \"data level\" of L1 or L2. \n",
    "\n",
    "Consequently, we will want to filter and group the metadata for a given reference designator to identify the relevant parameters. First, we query the preload database with the relevant metadata for a reference designator. Then, we filter the metadata for the science-relevant data streams based on the preload information. Then, we reduce the results by grouping by the stream parameter to get the stream-by-stream data, which will be useful when requesting data from OOINet for download. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0bc96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_levels = OOINet.get_parameter_data_levels(metadata)\n",
    "data_levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88385ad",
   "metadata": {},
   "source": [
    "Filter the metadata based on the data levels for **L1** & **L2** data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67ed39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_parameter_ids(pdId, pid_dict):\n",
    "    data_level = pid_dict.get(pdId)\n",
    "    if data_level is not None:\n",
    "        if data_level > 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e094827",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = metadata[\"pdId\"].apply(lambda x: filter_parameter_ids(x, data_levels))\n",
    "metadata = metadata[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0f4091",
   "metadata": {},
   "source": [
    "Groupby based on the reference designator - method - stream to get the unique values for each data stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f289c29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = metadata.groupby(by=[\"refdes\",\"method\",\"stream\"]).agg(lambda x: pd.unique(x.values.ravel()).tolist())\n",
    "metadata = metadata.reset_index()\n",
    "metadata = metadata.applymap(lambda x: x[0] if len(x) == 1 else x)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81a1e83",
   "metadata": {},
   "source": [
    "This returns all of the methods and streams which have scientific data. For PCO2W datasets, we want to drop the entries which have \"blank\" in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150f9f3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mask = metadata[\"stream\"].apply(lambda x: False if \"blank\" in x else True)\n",
    "metadata = metadata[mask]\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a1da7e",
   "metadata": {},
   "source": [
    "---\n",
    "## Deployment Information\n",
    "When we searched for datasets, it returned a table which listed the available deployment numbers for each of the datasets. We can get much more detailed information on the deployments for a particular reference designator by requesting the deployment information from OOINet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cb82f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployments = OOINet.get_deployments(refdes=refdes)\n",
    "deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc4e6c4",
   "metadata": {},
   "source": [
    "We'll go ahead and save the deployment data as a csv since it might be useful when working with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f103bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployments.to_csv(f\"../data/{refdes}_deployments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441fab9d",
   "metadata": {},
   "source": [
    "---\n",
    "## Vocab Information\n",
    "Additionally, if we are interested in more detailed information on the location that the reference designator is assigned to, we can request the vocab information for the given reference designator. The vocab information includes some of the \"**English_names**\" info we requested when searching for datasets, as well as instrument model, manufacturer, and the descriptive names for the reference designator location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9810c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = OOINet.get_vocab(refdes=refdes)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc162239",
   "metadata": {},
   "source": [
    "---\n",
    "## Calibration Information\n",
    "We can also request the calibration information for a given reference designator. Since individual instruments are swapped during each mooring deployment & recovery, the calibration coefficients for a reference designator are different for each deployment. The way OOI operates is that it loads all the available calibration coefficients for a given reference designator. Then, for each deployment, it finds the calibration coefficients with the most recent calibration date which most closely _precedes_ the start of the deployment. The result is a table, sorted by deployment number for a reference designator, with the uid of the specific instrument, its calibration coefficients, when the instrument was calibrated, and the source of the calibration coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764e3124",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrations = OOINet.get_calibrations(refdes, deployments)\n",
    "calibrations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591a9aa2",
   "metadata": {},
   "source": [
    "It is also possible to request the calibration history for a specific instrument by utilizing the **uid** of the instrument and using the lower-level ```_get_api``` method and ```OOINet.URLS``` attribute to construct your own request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2403e574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the calibration url and arguments to pass to the request\n",
    "cal_url = OOINet.URLS[\"cal\"]\n",
    "uid = \"CGINS-PCO2WB-C0096\" # This is unique to each instrument\n",
    "params = {\n",
    "    \"uid\": uid\n",
    "}\n",
    "\n",
    "# Make the request\n",
    "calInfo = OOINet._get_api(cal_url, params=params)\n",
    "\n",
    "# Put the data into a pandas dataframe, sorted by calibration date and coefficient name\n",
    "columns = [\"uid\", \"calCoef\", \"calDate\", \"value\", \"calFile\"]\n",
    "instrumentCals = pd.DataFrame(columns=columns)\n",
    "for c in calInfo[\"calibration\"]:\n",
    "    for cc in c[\"calData\"]:\n",
    "        instrumentCals = instrumentCals.append({\n",
    "            \"uid\": cc[\"assetUid\"],\n",
    "            \"calCoef\": cc[\"eventName\"],\n",
    "            \"calDate\": OOINet._convert_time(cc[\"eventStartTime\"]),\n",
    "            \"value\": cc[\"value\"],\n",
    "            \"calFile\": cc[\"dataSource\"]\n",
    "        }, ignore_index=True)\n",
    "instrumentCals.sort_values(by=[\"calDate\", \"calCoef\"], inplace=True)\n",
    "instrumentCals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a2afa2",
   "metadata": {},
   "source": [
    "---\n",
    "## Download Datasets\n",
    "The ultimate goal of the queries above were to identify what data streams(s) we are interested in, along with supporting metadata/calibration information, in order to request the to download. Now we want to be able to request those data streams and get the associated netCDF files. This process involves the following steps:\n",
    "1. Identify the methods and data streams for the selected reference designator\n",
    "2. Request the THREDDS server url for the data sets\n",
    "3. Get the catalog of datasets on the THREDDS server\n",
    "4. Parse the catalog for the desired netCDF files\n",
    "5. Download the identified netCDF files to a local directory\n",
    "\n",
    "Below, we script the above steps in order to download all of the available datasets. In the following section we will combine the data delivered via different methods (e.g. telemetered, recovered_host, recovered_inst) to generate a single combined dataset with the most complete data record available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469f731",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in metadata.index:\n",
    "    # Get the method and stream\n",
    "    method, stream = metadata.loc[row,\"method\"], metadata.loc[row, \"stream\"]\n",
    "    \n",
    "    if \"air\" in stream:\n",
    "        continue\n",
    "    \n",
    "   \n",
    "    # Get the THREDDS url\n",
    "    thredds_url = OOINet.get_thredds_url(refdes, method, stream)\n",
    "    \n",
    "    # Get the catalog\n",
    "    catalog = OOINet.get_thredds_catalog(thredds_url)\n",
    "    \n",
    "    # Remove unwanted datasets from the catalog\n",
    "    for dataset in catalog:\n",
    "        if \"blank\" in dataset:\n",
    "            catalog.remove(dataset)\n",
    "    \n",
    "    # Create a directory to save the data\n",
    "    save_dir = f\"../data/{refdes}/{method}/\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # Download the files to the save directory\n",
    "    OOINet.download_netCDF_files(catalog, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3140cb7",
   "metadata": {},
   "source": [
    "### Merge Datasets\n",
    "\n",
    "With the datasets downloaded to a local directory, we now want to combine the datasets delivered via the different methods into a single dataset. This dataset should have the most complete data record available for the given reference designator.\n",
    "\n",
    "\n",
    "#### Load the data\n",
    "First, load the downloaded data into xarray datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1cdf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the telemetered data sets\n",
    "telemetered_files = os.listdir(f\"../data/{refdes}/telemetered\")\n",
    "telemetered_files = sorted([f\"../data/{refdes}/telemetered/\" + f for f in telemetered_files if \"metbk\" not in f])\n",
    "telemetered_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ba8c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the recovered_host data sets\n",
    "recovered_host_files = os.listdir(f\"../data/{refdes}/recovered_host\")\n",
    "recovered_host_files = sorted([f\"../data/{refdes}/recovered_host/\" + f for f in recovered_host_files if \"metbk\" not in f])\n",
    "recovered_host_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3319d61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the recovered_inst data sets\n",
    "recovered_inst_files = os.listdir(f\"../data/{refdes}/recovered_inst\")\n",
    "recovered_inst_files = sorted([f\"../data/{refdes}/recovered_inst/\" + f for f in recovered_inst_files if \"metbk\" not in f])\n",
    "recovered_inst_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47619848",
   "metadata": {},
   "source": [
    "Load the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8251fca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2922ee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_datasets(datasets, refdes):\n",
    "    \"\"\"Opens datasets saved locally into an xarray dataset.\"\"\"\n",
    "    \n",
    "    OOINet.REFDES = refdes\n",
    "    \n",
    "    # check and remove any files which are malformed\n",
    "    # and remove the bad ones\n",
    "    netCDF_files = OOINet._check_files(datasets)\n",
    "    \n",
    "    # Load the datasets into a concatenated xarray DataSet\n",
    "    with ProgressBar():\n",
    "        print(\"\\n\"+f\"Loading netCDF_files for {OOINet.REFDES}:\")\n",
    "        ds = xr.open_mfdataset(netCDF_files, preprocess=OOINet._preprocess, parallel=True)\n",
    "        \n",
    "    # Add in the English name of the dataset\n",
    "    refdes = \"-\".join(ds.attrs[\"id\"].split(\"-\")[:4])\n",
    "    vocab = OOINet.get_vocab(refdes)\n",
    "    ds.attrs[\"Location_name\"] = \" \".join((vocab[\"tocL1\"].iloc[0],\n",
    "                                          vocab[\"tocL2\"].iloc[0],\n",
    "                                          vocab[\"tocL3\"].iloc[0]))    \n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db41239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tele_data = open_datasets(telemetered_files, refdes)\n",
    "host_data = open_datasets(recovered_host_files, refdes)\n",
    "inst_data = open_datasets(recovered_inst_files, refdes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b907b7",
   "metadata": {},
   "source": [
    "#### Optional Step: Process the dataset\n",
    "An additional step is to process the datasets to clean up the datasets and get rid of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f75c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/areed/Documents/OOI/oceanobservatories/ooi-data-explorations/python/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67dbe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ooi_data_explorations.uncabled import process_pco2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbf2b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_data = process_pco2w.pco2w_instrument(inst_data)\n",
    "host_data = process_pco2w.pco2w_datalogger(host_data)\n",
    "tele_data = process_pco2w.pco2w_datalogger(tele_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7b2a63",
   "metadata": {},
   "source": [
    "#### Merge data\n",
    "Now, we need to merge the data. First, we iterate through the data variables for each dataset, identify any which are unique to a given dataset, and broadcast them to the other datasets. This step is necessary to allow the datasets to combine. Once each dataset has the same data variables, we utilize ```xr.combine_first``` to combine the datasets. We assume that the instrument record, if available, is the best and most complete dataset and utilize the telemetered and recovered_host datasets to fill in the gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c28bb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to make sure each dataset has the same variables\n",
    "for var in tele_data.variables:\n",
    "    if var not in host_data.variables:\n",
    "        host_data[var] = tele_data[var].broadcast_like(host_data[\"time\"])\n",
    "        \n",
    "for var in host_data.variables:\n",
    "    if var not in tele_data.variables:\n",
    "        tele_data[var] = host_data[var].broadcast_like(tele_data[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05abe482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the telemetered dataset and host_dataset\n",
    "tele_host = tele_data.combine_first(host_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14fbb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in tele_host.variables:\n",
    "    if var not in inst_data.variables:\n",
    "        inst_data[var] = tele_host[var].broadcast_like(inst_data[\"time\"])\n",
    "\n",
    "for var in inst_data.variables:\n",
    "    if var not in tele_host.variables:\n",
    "        tele_host[var] = inst_data[var].broadcast_like(tele_host[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5943734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the instrument dataset with the combined telemetered-recovered_host dataset\n",
    "data = inst_data.combine_first(tele_host)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f915b4a",
   "metadata": {},
   "source": [
    "#### Save the results\n",
    "With the merged datasets, we can save the results locally as a netCDF file. However, some data variables contain improperly formatted datetimes and timestamps which will cause an error when saving. Generally, these data variables do not contain particularly useful information for a science-user and can be dropped before saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1ed7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5056e2",
   "metadata": {},
   "source": [
    "Save the data as a netCDF file using h5netcdf compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3887fc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_netcdf(f\"../data/{refdes}_combined.nc\", engine=\"h5netcdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2237a076",
   "metadata": {},
   "source": [
    "Close the dataset so it can be operated on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dabcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bb9cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\"../data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef8fc18",
   "metadata": {},
   "source": [
    "---\n",
    "## Annotations\n",
    "Annotations contain important qualitative assessments of data quality from the instrument operators. They may range from explanations for why data is missing for a given time period to information about biofouling or other data quality issues. Annotations can be downloaded from OOINet for a particular reference designator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5e5bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the annotations for each reference designator\n",
    "annotations = OOINet.get_annotations(refdes)\n",
    "annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f259695",
   "metadata": {},
   "source": [
    "Save the annotations to local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae63acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations.to_csv(f\"../data/{refdes}_annotations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fccdd9",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Explorer\n",
    "---\n",
    "The data from Data Explorer are hosted via ERDDAP. To interact with Data Explorer's ERDDAP, we'll utilize the python package ```erddapy```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bda6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from erddapy import ERDDAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf14192",
   "metadata": {},
   "source": [
    "**Data Explorer ERDDAP url**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c9318",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataExplorer = \"http://erddap.dataexplorer.oceanobservatories.org/erddap\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c378ba30",
   "metadata": {},
   "source": [
    "Connect to the Data Explorer ERDDAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b4e435",
   "metadata": {},
   "outputs": [],
   "source": [
    "erd = ERDDAP(server=dataExplorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e25c199",
   "metadata": {},
   "source": [
    "Search for ```PHSEN``` on the Irminger Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c62631",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_url = erd.get_search_url(search_for=\"gi03flma phsen\", \n",
    "                                protocol=\"tabledap\",\n",
    "                                response=\"csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7336b40d",
   "metadata": {},
   "source": [
    "Get the dataset ids for the available PHSEN datasets on the Irminger Sea Flanking Mooring A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7e6a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ids = pd.read_csv(search_url)[\"Dataset ID\"]\n",
    "dataset_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0332df18",
   "metadata": {},
   "source": [
    "Download the dataset from ERDDAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428eaec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the dataset id of the instrument you want to download\n",
    "dataset_id = \"ooi-gi03flma-ris01-04-phsenf000\"\n",
    "\n",
    "# Get the download url\n",
    "download_url = erd.get_download_url(dataset_id=dataset_id, \n",
    "                                    protocol=\"tabledap\",\n",
    "                                    response=\"opendap\")\n",
    "\n",
    "# Set up the parameters for the dataset request from the ERDDAP server\n",
    "erd.dataset_id = dataset_id\n",
    "erd.response = \"nc\"\n",
    "erd.protocol = \"tabledap\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9112a91",
   "metadata": {},
   "source": [
    "Open the requested dataset using ```xarray```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1fa83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = erd.to_xarray()\n",
    "ds = ds.swap_dims({\"obs\":\"time\"})\n",
    "ds = ds.sortby(\"time\")\n",
    "ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
